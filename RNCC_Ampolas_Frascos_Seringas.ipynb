{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNCC - Ampolas Frascos Seringas.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+/U244WKu6sV6eE4GcG7Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valerio-unifei/chemotherapy-safety/blob/main/RNCC_Ampolas_Frascos_Seringas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBoJWRGTi60p"
      },
      "source": [
        "# Identificação de Frascos, Ampolas e Seringas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HV5AT9Si-RV"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29dvzK19jLK_"
      },
      "source": [
        "### Baixando imagens do Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6dkD9WKjN-Q"
      },
      "source": [
        "A pasta de imagens já é subdivida nas classes para classificação:\n",
        "- Ampoule\n",
        "- BottleA\n",
        "- BottleB\n",
        "- Syringe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EXkxHNBo1CE",
        "outputId": "358924b7-5652-4148-d67f-b9c56e04d5a6"
      },
      "source": [
        "import gdown\n",
        "\n",
        "# Arquivo no Drive com dataset de imagens completo\n",
        "# https://drive.google.com/file/d/1n8LaNDE9GT1LYFG1jGcwbs3OMb4EMZj7/\n",
        "\n",
        "# baixando dataset no colab\n",
        "fullDS = 'https://drive.google.com/uc?id=1n8LaNDE9GT1LYFG1jGcwbs3OMb4EMZj7'\n",
        "gdown.download(fullDS, 'FullDataset.zip', quiet=False)\n",
        "\n",
        "#descompactando e apagando arquivo compactado\n",
        "!unzip -qq FullDataset.zip -d fullds\n",
        "!rm FullDataset.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1n8LaNDE9GT1LYFG1jGcwbs3OMb4EMZj7\n",
            "To: /content/FullDataset.zip\n",
            "286MB [00:03, 73.7MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFvVbonSjB7E"
      },
      "source": [
        "### Carregando Dataset no Formato Tensor para treinamento e validação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYzLLWF8n5h-",
        "outputId": "cd700bf3-3470-419d-c9dd-8f96d16e3dbc"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "directory = '/content/fullds'\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory\n",
        "image_size = (128, 128)\n",
        "batch_size = 32\n",
        "cores = 'rgb' \n",
        "#cores = 'grayscale'\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=directory,\n",
        "    validation_split=0.2, # 80%\n",
        "    subset='training',\n",
        "    seed=42,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    color_mode = cores,\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=directory,\n",
        "    validation_split=0.2, # 20%\n",
        "    subset='validation',\n",
        "    seed=42,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    color_mode = cores,\n",
        ")\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "\n",
        "train_ds = train_ds.prefetch(buffer_size=32)\n",
        "val_ds = val_ds.prefetch(buffer_size=32)\n",
        "\n",
        "class_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1423 files belonging to 4 classes.\n",
            "Using 1139 files for training.\n",
            "Found 1423 files belonging to 4 classes.\n",
            "Using 284 files for validation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ampoule', 'BottleA', 'BottleB', 'Syringe']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4GvQ5t7ja7V"
      },
      "source": [
        "## Modelo da rede neural por convolução para classificação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UAo5AQQjgDd"
      },
      "source": [
        "Esta rede tem como entrada uma imagem (largura, altura, cores) e saída o número de classes para classificar:\n",
        "- Ampoule (neuronio de saída 0)\n",
        "- BottleA (neuronio de saída 1)\n",
        "- BottleB (neuronio de saída 2)\n",
        "- Syringe (neuronio de saída 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rnWCxZwpSpZ",
        "outputId": "bf56f4c4-f6e0-4e52-9f35-62db78941edc"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(image_size[0], image_size[1], 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(len(class_names)))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 126, 126, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 63, 63, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 61, 61, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 64)        36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 50176)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                3211328   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 3,267,908\n",
            "Trainable params: 3,267,908\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gy0XheNkCtC"
      },
      "source": [
        "## Treinamento com 20 iterações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4K9LHO7phho",
        "outputId": "8698b3f1-e0bc-4ae7-fdb8-9520f550fe89"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_ds, epochs=20, validation_data=val_ds,)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "36/36 [==============================] - 34s 99ms/step - loss: 20.0389 - accuracy: 0.7191 - val_loss: 0.5538 - val_accuracy: 0.9049\n",
            "Epoch 2/20\n",
            "36/36 [==============================] - 9s 217ms/step - loss: 0.3826 - accuracy: 0.8982 - val_loss: 0.4662 - val_accuracy: 0.8979\n",
            "Epoch 3/20\n",
            "36/36 [==============================] - 9s 221ms/step - loss: 0.2046 - accuracy: 0.9385 - val_loss: 0.2097 - val_accuracy: 0.9261\n",
            "Epoch 4/20\n",
            "36/36 [==============================] - 10s 223ms/step - loss: 0.2606 - accuracy: 0.9368 - val_loss: 0.3371 - val_accuracy: 0.9225\n",
            "Epoch 5/20\n",
            "36/36 [==============================] - 9s 221ms/step - loss: 0.1853 - accuracy: 0.9526 - val_loss: 0.4014 - val_accuracy: 0.9261\n",
            "Epoch 6/20\n",
            "36/36 [==============================] - 9s 215ms/step - loss: 0.0892 - accuracy: 0.9781 - val_loss: 0.2014 - val_accuracy: 0.9331\n",
            "Epoch 7/20\n",
            "36/36 [==============================] - 9s 216ms/step - loss: 0.0688 - accuracy: 0.9816 - val_loss: 0.2265 - val_accuracy: 0.9437\n",
            "Epoch 8/20\n",
            "36/36 [==============================] - 9s 211ms/step - loss: 0.0356 - accuracy: 0.9921 - val_loss: 0.1541 - val_accuracy: 0.9542\n",
            "Epoch 9/20\n",
            "36/36 [==============================] - 9s 209ms/step - loss: 0.0363 - accuracy: 0.9895 - val_loss: 0.2001 - val_accuracy: 0.9366\n",
            "Epoch 10/20\n",
            "36/36 [==============================] - 9s 213ms/step - loss: 0.0259 - accuracy: 0.9956 - val_loss: 0.2025 - val_accuracy: 0.9507\n",
            "Epoch 11/20\n",
            "36/36 [==============================] - 9s 218ms/step - loss: 0.0089 - accuracy: 0.9982 - val_loss: 0.3933 - val_accuracy: 0.9472\n",
            "Epoch 12/20\n",
            "36/36 [==============================] - 9s 220ms/step - loss: 0.0236 - accuracy: 0.9956 - val_loss: 0.2216 - val_accuracy: 0.9542\n",
            "Epoch 13/20\n",
            "36/36 [==============================] - 9s 214ms/step - loss: 0.0216 - accuracy: 0.9939 - val_loss: 0.1999 - val_accuracy: 0.9577\n",
            "Epoch 14/20\n",
            "36/36 [==============================] - 9s 213ms/step - loss: 0.0988 - accuracy: 0.9816 - val_loss: 0.1892 - val_accuracy: 0.9401\n",
            "Epoch 15/20\n",
            "36/36 [==============================] - 9s 211ms/step - loss: 0.0883 - accuracy: 0.9710 - val_loss: 0.2849 - val_accuracy: 0.8944\n",
            "Epoch 16/20\n",
            "36/36 [==============================] - 9s 212ms/step - loss: 0.1672 - accuracy: 0.9500 - val_loss: 0.3260 - val_accuracy: 0.9331\n",
            "Epoch 17/20\n",
            "36/36 [==============================] - 9s 220ms/step - loss: 0.1020 - accuracy: 0.9754 - val_loss: 0.2723 - val_accuracy: 0.9225\n",
            "Epoch 18/20\n",
            "36/36 [==============================] - 10s 224ms/step - loss: 0.0564 - accuracy: 0.9807 - val_loss: 0.2439 - val_accuracy: 0.9261\n",
            "Epoch 19/20\n",
            "36/36 [==============================] - 9s 216ms/step - loss: 0.0217 - accuracy: 0.9930 - val_loss: 0.3448 - val_accuracy: 0.9542\n",
            "Epoch 20/20\n",
            "36/36 [==============================] - 9s 211ms/step - loss: 0.0064 - accuracy: 0.9974 - val_loss: 0.2704 - val_accuracy: 0.9542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdS8qm9VkGr4"
      },
      "source": [
        "# Validação de Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOkqmapLkJog"
      },
      "source": [
        "Matriz de confusão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh3lVR4hp2Xi",
        "outputId": "daec6266-f66b-4079-aebe-36da6b8459c4"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "predictions = np.array([])\n",
        "labels =  np.array([])\n",
        "#dados de validação\n",
        "for x, y in val_ds:\n",
        "  predictions = np.concatenate([predictions, np.argmax(model.predict(x), axis=-1)])\n",
        "  labels = np.concatenate([labels,y.numpy()])\n",
        "#dados de treinamento\n",
        "for x, y in train_ds:\n",
        "  predictions = np.concatenate([predictions, np.argmax(model.predict(x), axis=-1)])\n",
        "  labels = np.concatenate([labels,y.numpy()])\n",
        "\n",
        "#matriz de confusão do conjunto completo\n",
        "tf.math.confusion_matrix(labels=labels, predictions=predictions).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  34,    2,    1,    4],\n",
              "       [   0,   77,    0,    1],\n",
              "       [   0,    1,   66,    4],\n",
              "       [   0,    0,    1, 1232]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    }
  ]
}